{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE285 Assignment 1: Neural Network in NumPy\n",
    "\n",
    "Use this notebook to build your neural network by implementing the following functions in the python files under `ece285/algorithms` directory:\n",
    "\n",
    "1. `linear.py`\n",
    "2. `relu.py`\n",
    "3. `softmax.py`\n",
    "4. `loss_func.py`\n",
    "\n",
    "You will be testing your 2 layer neural network implementation on a toy dataset.\n",
    "\n",
    "TO SUBMIT: PDF of this notebook with all the required outputs and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ece285.layers.sequential import Sequential\n",
    "from ece285.layers.linear import Linear\n",
    "from ece285.layers.relu import ReLU\n",
    "from ece285.layers.softmax import Softmax\n",
    "from ece285.layers.loss_func import CrossEntropyLoss\n",
    "from ece285.utils.optimizer import SGD\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the class Sequential as implemented in the file assignment2/layers/sequential.py to build a layer by layer model of our neural network. Below we initialize the toy model and the toy random data that you will use to develop your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3  # Output\n",
    "num_inputs = 10  # N\n",
    "\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    l1 = Linear(input_size, hidden_size)\n",
    "    l2 = Linear(hidden_size, num_classes)\n",
    "\n",
    "    r1 = ReLU()\n",
    "    softmax = Softmax()\n",
    "    return Sequential([l1, r1, l2, softmax])\n",
    "\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(0)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.random.randint(num_classes, size=num_inputs)\n",
    "    # y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass: Compute Scores (20%)\n",
    "Implement the forward functions in Linear, Relu and Softmax layers and get the output by passing our toy data X\n",
    "The output must match the given output scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[0.33333514 0.33333826 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.33333509 0.33333829 0.33332662]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332661]\n",
      " [0.33333512 0.33333827 0.33332661]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332662]]\n",
      "\n",
      "correct scores:\n",
      "[[0.33333514 0.33333826 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332661]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.3333351  0.33333828 0.33332662]\n",
      " [0.33333509 0.33333829 0.33332662]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332661]\n",
      " [0.33333512 0.33333827 0.33332661]\n",
      " [0.33333508 0.33333829 0.33332662]\n",
      " [0.33333511 0.33333828 0.33332662]]\n",
      "Difference between your scores and correct scores:\n",
      "8.799388540037256e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.forward(X)\n",
    "print(\"Your scores:\")\n",
    "print(scores)\n",
    "print()\n",
    "print(\"correct scores:\")\n",
    "correct_scores = np.asarray(\n",
    "    [\n",
    "        [0.33333514, 0.33333826, 0.33332661],\n",
    "        [0.3333351, 0.33333828, 0.33332661],\n",
    "        [0.3333351, 0.33333828, 0.33332662],\n",
    "        [0.3333351, 0.33333828, 0.33332662],\n",
    "        [0.33333509, 0.33333829, 0.33332662],\n",
    "        [0.33333508, 0.33333829, 0.33332662],\n",
    "        [0.33333511, 0.33333828, 0.33332661],\n",
    "        [0.33333512, 0.33333827, 0.33332661],\n",
    "        [0.33333508, 0.33333829, 0.33332662],\n",
    "        [0.33333511, 0.33333828, 0.33332662],\n",
    "    ]\n",
    ")\n",
    "print(correct_scores)\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print(\"Difference between your scores and correct scores:\")\n",
    "print(np.sum(np.abs(scores - correct_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass: Compute loss given the output scores from the previous step (10%)\n",
    "Implement the forward function in the loss_func.py file, and output the loss value. The loss value must match the given loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.098612723362578\n",
      "Difference between your loss and correct loss:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "Loss = CrossEntropyLoss()\n",
    "loss = Loss.forward(scores, y)\n",
    "correct_loss = 1.098612723362578\n",
    "print(loss)\n",
    "# should be very small, we get < 1e-12\n",
    "print(\"Difference between your loss and correct loss:\")\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass (40%)\n",
    "Implement the rest of the functions in the given files. Specifically, implement the backward function in all the 4 files as mentioned in the files. Note: No backward function in the softmax file, the gradient for softmax is jointly calculated with the cross entropy loss in the loss_func.backward function.\n",
    "\n",
    "You will use the chain rule to calculate gradient individually for each layer. You can assume that this calculated gradeint then is passed to the next layers in a reversed manner due to the Sequential implementation. So all you need to worry about is implementing the gradient for the current layer and multiply it will the incoming gradient (passed to the backward function as dout) to calculate the total gradient for the parameters of that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "(10,)\n",
      "(10, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# No need to edit anything in this block ( 20% of the above 40% )\n",
    "net.backward(Loss.backward())\n",
    "\n",
    "gradients = []\n",
    "for module in net._modules:\n",
    "    for para, grad in zip(module.parameters, module.grads):\n",
    "        assert grad is not None, \"No Gradient\"\n",
    "        # Print gradients of the linear layer\n",
    "        print(grad.shape)\n",
    "        gradients.append(grad)\n",
    "\n",
    "# Check shapes of your gradient. Note that only the linear layer has parameters\n",
    "# (4, 10) -> Layer 1 W\n",
    "# (10,)   -> Layer 1 b\n",
    "# (10, 3) -> Layer 2 W\n",
    "# (3,)    -> Layer 2 b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in Gradient values 5.665200831813677e-12\n"
     ]
    }
   ],
   "source": [
    "# No need to edit anything in this block ( 20% of the above 40% )\n",
    "# Now we check the values for these gradients. Here are the values for these gradients, Below we calculate the\n",
    "# difference, you must get difference < 1e-10\n",
    "grad_w1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            -6.24320917e-05,\n",
    "            3.41037180e-06,\n",
    "            -1.69125969e-05,\n",
    "            2.41514079e-05,\n",
    "            3.88697976e-06,\n",
    "            7.63842314e-05,\n",
    "            -8.88925758e-05,\n",
    "            3.34909890e-05,\n",
    "            -1.42758303e-05,\n",
    "            -4.74748560e-06,\n",
    "        ],\n",
    "        [\n",
    "            -7.16182867e-05,\n",
    "            4.63270039e-06,\n",
    "            -2.20344270e-05,\n",
    "            -2.72027034e-06,\n",
    "            6.52903437e-07,\n",
    "            8.97294847e-05,\n",
    "            -1.05981609e-04,\n",
    "            4.15825391e-05,\n",
    "            -2.12210745e-05,\n",
    "            3.06061658e-05,\n",
    "        ],\n",
    "        [\n",
    "            -1.69074923e-05,\n",
    "            -8.83185056e-06,\n",
    "            3.10730840e-05,\n",
    "            1.23010428e-05,\n",
    "            5.25830316e-05,\n",
    "            -7.82980115e-06,\n",
    "            3.02117990e-05,\n",
    "            -3.37645284e-05,\n",
    "            6.17276346e-05,\n",
    "            -1.10735656e-05,\n",
    "        ],\n",
    "        [\n",
    "            -4.35902272e-05,\n",
    "            3.71512704e-06,\n",
    "            -1.66837877e-05,\n",
    "            2.54069557e-06,\n",
    "            -4.33258099e-06,\n",
    "            5.72310022e-05,\n",
    "            -6.94881762e-05,\n",
    "            2.92408329e-05,\n",
    "            -1.89369767e-05,\n",
    "            2.01692516e-05,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "grad_b1 = np.array(\n",
    "    [\n",
    "        -2.27150209e-06,\n",
    "        5.14674340e-07,\n",
    "        -2.04284403e-06,\n",
    "        6.08849787e-07,\n",
    "        -1.92177796e-06,\n",
    "        3.92085824e-06,\n",
    "        -5.40772636e-06,\n",
    "        2.93354593e-06,\n",
    "        -3.14568138e-06,\n",
    "        5.27501592e-11,\n",
    "    ]\n",
    ")\n",
    "\n",
    "grad_w2 = np.array(\n",
    "    [\n",
    "        [1.28932983e-04, 1.19946731e-04, -2.48879714e-04],\n",
    "        [1.08784150e-04, 1.55140199e-04, -2.63924349e-04],\n",
    "        [6.96017544e-05, 1.42748410e-04, -2.12350164e-04],\n",
    "        [9.92512487e-05, 1.73257611e-04, -2.72508860e-04],\n",
    "        [2.05484895e-05, 4.96161144e-05, -7.01646039e-05],\n",
    "        [8.20539510e-05, 9.37063861e-05, -1.75760337e-04],\n",
    "        [2.45831715e-05, 8.74369112e-05, -1.12020083e-04],\n",
    "        [1.34073379e-04, 1.86253064e-04, -3.20326443e-04],\n",
    "        [8.86473128e-05, 2.35554414e-04, -3.24201726e-04],\n",
    "        [3.57433149e-05, 1.91164061e-04, -2.26907376e-04],\n",
    "    ]\n",
    ")\n",
    "\n",
    "grad_b2 = np.array([-0.1666649, 0.13333828, 0.03332662])\n",
    "\n",
    "difference = (\n",
    "    np.sum(np.abs(gradients[0] - grad_w1))\n",
    "    + np.sum(np.abs(gradients[1] - grad_b1))\n",
    "    + np.sum(np.abs(gradients[2] - grad_w2))\n",
    ")\n",
    "+np.sum(np.abs(gradients[3] - grad_b2))\n",
    "print(\"Difference in Gradient values\", difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the complete network on the toy data. (30%)\n",
    "\n",
    "To train the network we will use stochastic gradient descent (SGD), we have implemented the optimizer for you. You do not implement any more functions in the python files. Below we implement the training procedure, you should get yourself familiar with the training process. Specifically looking at which functions to call and when.\n",
    "\n",
    "Once you have implemented the method and tested various parts in the above blocks, run the code below to train a two-layer network on toy data. You should see your training loss decrease below 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 1.098613\n",
      "Epoch Loss: 1.094024\n",
      "Epoch Loss: 1.089737\n",
      "Epoch Loss: 1.085733\n",
      "Epoch Loss: 1.081994\n",
      "Epoch Loss: 1.078505\n",
      "Epoch Loss: 1.075248\n",
      "Epoch Loss: 1.072208\n",
      "Epoch Loss: 1.069372\n",
      "Epoch Loss: 1.066726\n",
      "Epoch Loss: 1.064257\n",
      "Epoch Loss: 1.061952\n",
      "Epoch Loss: 1.059799\n",
      "Epoch Loss: 1.057785\n",
      "Epoch Loss: 1.055899\n",
      "Epoch Loss: 1.054124\n",
      "Epoch Loss: 1.052445\n",
      "Epoch Loss: 1.050839\n",
      "Epoch Loss: 1.049277\n",
      "Epoch Loss: 1.047713\n",
      "Epoch Loss: 1.046081\n",
      "Epoch Loss: 1.044279\n",
      "Epoch Loss: 1.042147\n",
      "Epoch Loss: 1.039435\n",
      "Epoch Loss: 1.035774\n",
      "Epoch Loss: 1.030637\n",
      "Epoch Loss: 1.023365\n",
      "Epoch Loss: 1.013337\n",
      "Epoch Loss: 1.000400\n",
      "Epoch Loss: 0.985502\n",
      "Epoch Loss: 0.970780\n",
      "Epoch Loss: 0.958224\n",
      "Epoch Loss: 0.948146\n",
      "Epoch Loss: 0.939687\n",
      "Epoch Loss: 0.932411\n",
      "Epoch Loss: 0.926597\n",
      "Epoch Loss: 0.920919\n",
      "Epoch Loss: 0.914630\n",
      "Epoch Loss: 0.908446\n",
      "Epoch Loss: 0.902708\n",
      "Epoch Loss: 0.895794\n",
      "Epoch Loss: 0.889273\n",
      "Epoch Loss: 0.882132\n",
      "Epoch Loss: 0.875647\n",
      "Epoch Loss: 0.870536\n",
      "Epoch Loss: 0.861775\n",
      "Epoch Loss: 0.855117\n",
      "Epoch Loss: 0.848626\n",
      "Epoch Loss: 0.839904\n",
      "Epoch Loss: 0.832706\n",
      "Epoch Loss: 0.827365\n",
      "Epoch Loss: 0.815973\n",
      "Epoch Loss: 0.810844\n",
      "Epoch Loss: 0.802194\n",
      "Epoch Loss: 0.790601\n",
      "Epoch Loss: 0.783502\n",
      "Epoch Loss: 0.770632\n",
      "Epoch Loss: 0.760160\n",
      "Epoch Loss: 0.749472\n",
      "Epoch Loss: 0.739295\n",
      "Epoch Loss: 0.732478\n",
      "Epoch Loss: 0.719142\n",
      "Epoch Loss: 0.713844\n",
      "Epoch Loss: 0.700037\n",
      "Epoch Loss: 0.693629\n",
      "Epoch Loss: 0.689958\n",
      "Epoch Loss: 0.674298\n",
      "Epoch Loss: 0.659023\n",
      "Epoch Loss: 0.647852\n",
      "Epoch Loss: 0.638275\n",
      "Epoch Loss: 0.628526\n",
      "Epoch Loss: 0.622772\n",
      "Epoch Loss: 0.617140\n",
      "Epoch Loss: 0.608889\n",
      "Epoch Loss: 0.601315\n",
      "Epoch Loss: 0.590965\n",
      "Epoch Loss: 0.588483\n",
      "Epoch Loss: 0.580111\n",
      "Epoch Loss: 0.580237\n",
      "Epoch Loss: 0.579590\n",
      "Epoch Loss: 0.575265\n",
      "Epoch Loss: 0.583167\n",
      "Epoch Loss: 0.569299\n",
      "Epoch Loss: 0.567051\n",
      "Epoch Loss: 0.558750\n",
      "Epoch Loss: 0.570455\n",
      "Epoch Loss: 0.550303\n",
      "Epoch Loss: 0.556647\n",
      "Epoch Loss: 0.528476\n",
      "Epoch Loss: 0.523165\n",
      "Epoch Loss: 0.503276\n",
      "Epoch Loss: 0.508981\n",
      "Epoch Loss: 0.493973\n",
      "Epoch Loss: 0.512211\n",
      "Epoch Loss: 0.477031\n",
      "Epoch Loss: 0.483331\n",
      "Epoch Loss: 0.456475\n",
      "Epoch Loss: 0.472357\n",
      "Epoch Loss: 0.443569\n",
      "Epoch Loss: 0.454687\n",
      "Epoch Loss: 0.429413\n",
      "Epoch Loss: 0.441666\n",
      "Epoch Loss: 0.413838\n",
      "Epoch Loss: 0.443494\n",
      "Epoch Loss: 0.407146\n",
      "Epoch Loss: 0.444991\n",
      "Epoch Loss: 0.403453\n",
      "Epoch Loss: 0.436572\n",
      "Epoch Loss: 0.397764\n",
      "Epoch Loss: 0.439797\n",
      "Epoch Loss: 0.422846\n",
      "Epoch Loss: 0.526893\n",
      "Epoch Loss: 0.553876\n",
      "Epoch Loss: 0.728167\n",
      "Epoch Loss: 0.582263\n",
      "Epoch Loss: 0.547702\n",
      "Epoch Loss: 0.416240\n",
      "Epoch Loss: 0.453693\n",
      "Epoch Loss: 0.381732\n",
      "Epoch Loss: 0.362521\n",
      "Epoch Loss: 0.324897\n",
      "Epoch Loss: 0.338047\n",
      "Epoch Loss: 0.342623\n",
      "Epoch Loss: 0.326819\n",
      "Epoch Loss: 0.392589\n",
      "Epoch Loss: 0.292861\n",
      "Epoch Loss: 0.314854\n",
      "Epoch Loss: 0.235608\n",
      "Epoch Loss: 0.227234\n",
      "Epoch Loss: 0.207276\n",
      "Epoch Loss: 0.220121\n",
      "Epoch Loss: 0.208397\n",
      "Epoch Loss: 0.249017\n",
      "Epoch Loss: 0.226502\n",
      "Epoch Loss: 0.284894\n",
      "Epoch Loss: 0.233826\n",
      "Epoch Loss: 0.259128\n",
      "Epoch Loss: 0.195131\n",
      "Epoch Loss: 0.177242\n",
      "Epoch Loss: 0.156995\n",
      "Epoch Loss: 0.147287\n",
      "Epoch Loss: 0.142642\n",
      "Epoch Loss: 0.139515\n",
      "Epoch Loss: 0.136126\n",
      "Epoch Loss: 0.135978\n",
      "Epoch Loss: 0.130053\n",
      "Epoch Loss: 0.127937\n",
      "Epoch Loss: 0.122477\n",
      "Epoch Loss: 0.120641\n",
      "Epoch Loss: 0.118350\n",
      "Epoch Loss: 0.117382\n",
      "Epoch Loss: 0.112601\n",
      "Epoch Loss: 0.111264\n",
      "Epoch Loss: 0.106784\n",
      "Epoch Loss: 0.105601\n",
      "Epoch Loss: 0.105261\n",
      "Epoch Loss: 0.103690\n",
      "Epoch Loss: 0.099740\n",
      "Epoch Loss: 0.097608\n",
      "Epoch Loss: 0.094972\n",
      "Epoch Loss: 0.094597\n",
      "Epoch Loss: 0.093439\n",
      "Epoch Loss: 0.092069\n",
      "Epoch Loss: 0.090610\n",
      "Epoch Loss: 0.087682\n",
      "Epoch Loss: 0.085767\n",
      "Epoch Loss: 0.084664\n",
      "Epoch Loss: 0.083181\n",
      "Epoch Loss: 0.082531\n",
      "Epoch Loss: 0.081955\n",
      "Epoch Loss: 0.079550\n",
      "Epoch Loss: 0.078298\n",
      "Epoch Loss: 0.077149\n",
      "Epoch Loss: 0.075943\n",
      "Epoch Loss: 0.074927\n",
      "Epoch Loss: 0.074027\n",
      "Epoch Loss: 0.074400\n",
      "Epoch Loss: 0.072842\n",
      "Epoch Loss: 0.071362\n",
      "Epoch Loss: 0.070140\n",
      "Epoch Loss: 0.069109\n",
      "Epoch Loss: 0.068189\n",
      "Epoch Loss: 0.067438\n",
      "Epoch Loss: 0.066542\n",
      "Epoch Loss: 0.065878\n",
      "Epoch Loss: 0.065619\n",
      "Epoch Loss: 0.065236\n",
      "Epoch Loss: 0.064023\n",
      "Epoch Loss: 0.063155\n",
      "Epoch Loss: 0.062142\n",
      "Epoch Loss: 0.061411\n",
      "Epoch Loss: 0.060913\n",
      "Epoch Loss: 0.060049\n",
      "Epoch Loss: 0.059368\n",
      "Epoch Loss: 0.059265\n",
      "Epoch Loss: 0.058653\n",
      "Epoch Loss: 0.057884\n",
      "Epoch Loss: 0.057724\n",
      "Epoch Loss: 0.056640\n",
      "Epoch Loss: 0.055911\n",
      "Epoch Loss: 0.055581\n",
      "Epoch Loss: 0.054974\n",
      "Epoch Loss: 0.054336\n",
      "Epoch Loss: 0.053731\n",
      "Epoch Loss: 0.053237\n",
      "Epoch Loss: 0.052698\n",
      "Epoch Loss: 0.052610\n",
      "Epoch Loss: 0.052149\n",
      "Epoch Loss: 0.051605\n",
      "Epoch Loss: 0.051018\n",
      "Epoch Loss: 0.050427\n",
      "Epoch Loss: 0.050176\n",
      "Epoch Loss: 0.050089\n",
      "Epoch Loss: 0.049610\n",
      "Epoch Loss: 0.048936\n",
      "Epoch Loss: 0.048407\n",
      "Epoch Loss: 0.048042\n",
      "Epoch Loss: 0.047693\n",
      "Epoch Loss: 0.047232\n",
      "Epoch Loss: 0.047006\n",
      "Epoch Loss: 0.046590\n",
      "Epoch Loss: 0.046342\n",
      "Epoch Loss: 0.045836\n",
      "Epoch Loss: 0.045452\n",
      "Epoch Loss: 0.045112\n",
      "Epoch Loss: 0.044795\n",
      "Epoch Loss: 0.044464\n",
      "Epoch Loss: 0.044038\n",
      "Epoch Loss: 0.043724\n",
      "Epoch Loss: 0.043653\n",
      "Epoch Loss: 0.043454\n",
      "Epoch Loss: 0.042933\n",
      "Epoch Loss: 0.042565\n",
      "Epoch Loss: 0.042280\n",
      "Epoch Loss: 0.041901\n",
      "Epoch Loss: 0.041709\n",
      "Epoch Loss: 0.041528\n",
      "Epoch Loss: 0.041169\n",
      "Epoch Loss: 0.040963\n",
      "Epoch Loss: 0.040683\n",
      "Epoch Loss: 0.040507\n",
      "Epoch Loss: 0.040178\n",
      "Epoch Loss: 0.039961\n",
      "Epoch Loss: 0.039609\n",
      "Epoch Loss: 0.039391\n",
      "Epoch Loss: 0.039159\n",
      "Epoch Loss: 0.038825\n",
      "Epoch Loss: 0.038546\n",
      "Epoch Loss: 0.038335\n",
      "Epoch Loss: 0.038039\n",
      "Epoch Loss: 0.037768\n",
      "Epoch Loss: 0.037602\n",
      "Epoch Loss: 0.037523\n",
      "Epoch Loss: 0.037224\n",
      "Epoch Loss: 0.036954\n",
      "Epoch Loss: 0.036836\n",
      "Epoch Loss: 0.036784\n",
      "Epoch Loss: 0.036457\n",
      "Epoch Loss: 0.036186\n",
      "Epoch Loss: 0.035966\n",
      "Epoch Loss: 0.035764\n",
      "Epoch Loss: 0.035549\n",
      "Epoch Loss: 0.035444\n",
      "Epoch Loss: 0.035394\n",
      "Epoch Loss: 0.035143\n",
      "Epoch Loss: 0.034874\n",
      "Epoch Loss: 0.034649\n",
      "Epoch Loss: 0.034474\n",
      "Epoch Loss: 0.034245\n",
      "Epoch Loss: 0.034032\n",
      "Epoch Loss: 0.033865\n",
      "Epoch Loss: 0.033680\n",
      "Epoch Loss: 0.033470\n",
      "Epoch Loss: 0.033298\n",
      "Epoch Loss: 0.033177\n",
      "Epoch Loss: 0.032968\n",
      "Epoch Loss: 0.032779\n",
      "Epoch Loss: 0.032638\n",
      "Epoch Loss: 0.032442\n",
      "Epoch Loss: 0.032259\n",
      "Epoch Loss: 0.032117\n",
      "Epoch Loss: 0.032015\n",
      "Epoch Loss: 0.032165\n",
      "Epoch Loss: 0.031888\n",
      "Epoch Loss: 0.031721\n",
      "Epoch Loss: 0.031566\n",
      "Epoch Loss: 0.031528\n",
      "Epoch Loss: 0.031292\n",
      "Epoch Loss: 0.031094\n",
      "Epoch Loss: 0.030956\n",
      "Epoch Loss: 0.030888\n",
      "Epoch Loss: 0.030697\n",
      "Epoch Loss: 0.030521\n",
      "Epoch Loss: 0.030383\n",
      "Epoch Loss: 0.030241\n",
      "Epoch Loss: 0.030111\n",
      "Epoch Loss: 0.029963\n",
      "Epoch Loss: 0.029830\n",
      "Epoch Loss: 0.029668\n",
      "Epoch Loss: 0.029528\n",
      "Epoch Loss: 0.029408\n",
      "Epoch Loss: 0.029255\n",
      "Epoch Loss: 0.029114\n",
      "Epoch Loss: 0.029005\n",
      "Epoch Loss: 0.028858\n",
      "Epoch Loss: 0.028718\n",
      "Epoch Loss: 0.028612\n",
      "Epoch Loss: 0.028480\n",
      "Epoch Loss: 0.028342\n",
      "Epoch Loss: 0.028228\n",
      "Epoch Loss: 0.028112\n",
      "Epoch Loss: 0.028042\n",
      "Epoch Loss: 0.028009\n",
      "Epoch Loss: 0.027869\n",
      "Epoch Loss: 0.027722\n",
      "Epoch Loss: 0.027666\n",
      "Epoch Loss: 0.027744\n",
      "Epoch Loss: 0.027542\n",
      "Epoch Loss: 0.027385\n",
      "Epoch Loss: 0.027244\n",
      "Epoch Loss: 0.027115\n",
      "Epoch Loss: 0.027015\n",
      "Epoch Loss: 0.026908\n",
      "Epoch Loss: 0.026801\n",
      "Epoch Loss: 0.026707\n",
      "Epoch Loss: 0.026660\n",
      "Epoch Loss: 0.026527\n",
      "Epoch Loss: 0.026404\n",
      "Epoch Loss: 0.026311\n",
      "Epoch Loss: 0.026192\n",
      "Epoch Loss: 0.026078\n",
      "Epoch Loss: 0.025984\n",
      "Epoch Loss: 0.025883\n",
      "Epoch Loss: 0.025771\n",
      "Epoch Loss: 0.025673\n",
      "Epoch Loss: 0.025583\n",
      "Epoch Loss: 0.025474\n",
      "Epoch Loss: 0.025383\n",
      "Epoch Loss: 0.025316\n",
      "Epoch Loss: 0.025206\n",
      "Epoch Loss: 0.025106\n",
      "Epoch Loss: 0.025025\n",
      "Epoch Loss: 0.024921\n",
      "Epoch Loss: 0.024822\n",
      "Epoch Loss: 0.024753\n",
      "Epoch Loss: 0.024686\n",
      "Epoch Loss: 0.024655\n",
      "Epoch Loss: 0.024557\n",
      "Epoch Loss: 0.024513\n",
      "Epoch Loss: 0.024400\n",
      "Epoch Loss: 0.024295\n",
      "Epoch Loss: 0.024202\n",
      "Epoch Loss: 0.024120\n",
      "Epoch Loss: 0.024023\n",
      "Epoch Loss: 0.023949\n",
      "Epoch Loss: 0.023882\n",
      "Epoch Loss: 0.023873\n",
      "Epoch Loss: 0.023803\n",
      "Epoch Loss: 0.023696\n",
      "Epoch Loss: 0.023608\n",
      "Epoch Loss: 0.023513\n",
      "Epoch Loss: 0.023421\n",
      "Epoch Loss: 0.023337\n",
      "Epoch Loss: 0.023263\n",
      "Epoch Loss: 0.023175\n",
      "Epoch Loss: 0.023089\n",
      "Epoch Loss: 0.023058\n",
      "Epoch Loss: 0.022986\n",
      "Epoch Loss: 0.022898\n",
      "Epoch Loss: 0.022820\n",
      "Epoch Loss: 0.022764\n",
      "Epoch Loss: 0.022686\n",
      "Epoch Loss: 0.022602\n",
      "Epoch Loss: 0.022536\n",
      "Epoch Loss: 0.022452\n",
      "Epoch Loss: 0.022371\n",
      "Epoch Loss: 0.022310\n",
      "Epoch Loss: 0.022230\n",
      "Epoch Loss: 0.022152\n",
      "Epoch Loss: 0.022091\n",
      "Epoch Loss: 0.022017\n",
      "Epoch Loss: 0.021940\n",
      "Epoch Loss: 0.021877\n",
      "Epoch Loss: 0.021808\n",
      "Epoch Loss: 0.021733\n",
      "Epoch Loss: 0.021676\n",
      "Epoch Loss: 0.021642\n",
      "Epoch Loss: 0.021625\n",
      "Epoch Loss: 0.021543\n",
      "Epoch Loss: 0.021474\n",
      "Epoch Loss: 0.021424\n",
      "Epoch Loss: 0.021348\n",
      "Epoch Loss: 0.021282\n",
      "Epoch Loss: 0.021211\n",
      "Epoch Loss: 0.021137\n",
      "Epoch Loss: 0.021074\n",
      "Epoch Loss: 0.021011\n",
      "Epoch Loss: 0.020940\n",
      "Epoch Loss: 0.020878\n",
      "Epoch Loss: 0.020819\n",
      "Epoch Loss: 0.020751\n",
      "Epoch Loss: 0.020688\n",
      "Epoch Loss: 0.020633\n",
      "Epoch Loss: 0.020566\n",
      "Epoch Loss: 0.020504\n",
      "Epoch Loss: 0.020451\n",
      "Epoch Loss: 0.020385\n",
      "Epoch Loss: 0.020313\n",
      "Epoch Loss: 0.020251\n",
      "Epoch Loss: 0.020197\n",
      "Epoch Loss: 0.020169\n",
      "Epoch Loss: 0.020110\n",
      "Epoch Loss: 0.020046\n",
      "Epoch Loss: 0.019971\n",
      "Epoch Loss: 0.019898\n",
      "Epoch Loss: 0.019833\n",
      "Epoch Loss: 0.019767\n",
      "Epoch Loss: 0.019698\n",
      "Epoch Loss: 0.019641\n",
      "Epoch Loss: 0.019610\n",
      "Epoch Loss: 0.019540\n",
      "Epoch Loss: 0.019472\n",
      "Epoch Loss: 0.019407\n",
      "Epoch Loss: 0.019350\n",
      "Epoch Loss: 0.019300\n",
      "Epoch Loss: 0.019235\n",
      "Epoch Loss: 0.019172\n",
      "Epoch Loss: 0.019116\n",
      "Epoch Loss: 0.019053\n",
      "Epoch Loss: 0.018993\n",
      "Epoch Loss: 0.018957\n",
      "Epoch Loss: 0.018905\n",
      "Epoch Loss: 0.018855\n",
      "Epoch Loss: 0.018792\n",
      "Epoch Loss: 0.018782\n",
      "Epoch Loss: 0.018733\n",
      "Epoch Loss: 0.018664\n",
      "Epoch Loss: 0.018605\n",
      "Epoch Loss: 0.018545\n",
      "Epoch Loss: 0.018484\n",
      "Epoch Loss: 0.018426\n",
      "Epoch Loss: 0.018374\n",
      "Epoch Loss: 0.018317\n",
      "Epoch Loss: 0.018260\n",
      "Epoch Loss: 0.018212\n",
      "Epoch Loss: 0.018156\n",
      "Epoch Loss: 0.018101\n",
      "Epoch Loss: 0.018054\n",
      "Epoch Loss: 0.018001\n",
      "Epoch Loss: 0.017947\n",
      "Epoch Loss: 0.017900\n",
      "Epoch Loss: 0.017862\n",
      "Epoch Loss: 0.017808\n",
      "Epoch Loss: 0.017761\n",
      "Epoch Loss: 0.017711\n",
      "Epoch Loss: 0.017659\n",
      "Epoch Loss: 0.017613\n",
      "Epoch Loss: 0.017564\n",
      "Epoch Loss: 0.017513\n",
      "Epoch Loss: 0.017468\n",
      "Epoch Loss: 0.017421\n",
      "Epoch Loss: 0.017371\n",
      "Epoch Loss: 0.017327\n",
      "Epoch Loss: 0.017281\n",
      "Epoch Loss: 0.017232\n",
      "Epoch Loss: 0.017188\n",
      "Epoch Loss: 0.017143\n",
      "Epoch Loss: 0.017096\n",
      "Epoch Loss: 0.017052\n",
      "Epoch Loss: 0.017009\n",
      "Epoch Loss: 0.016962\n",
      "Epoch Loss: 0.016919\n",
      "Epoch Loss: 0.016877\n",
      "Epoch Loss: 0.016831\n",
      "Epoch Loss: 0.016825\n",
      "Epoch Loss: 0.016779\n",
      "Epoch Loss: 0.016733\n",
      "Epoch Loss: 0.016688\n",
      "Epoch Loss: 0.016648\n",
      "Epoch Loss: 0.016615\n",
      "Epoch Loss: 0.016571\n",
      "Epoch Loss: 0.016538\n",
      "Epoch Loss: 0.016511\n",
      "Epoch Loss: 0.016489\n",
      "Epoch Loss: 0.016449\n",
      "Epoch Loss: 0.016401\n",
      "Epoch Loss: 0.016354\n",
      "Epoch Loss: 0.016312\n",
      "Epoch Loss: 0.016269\n",
      "Epoch Loss: 0.016226\n",
      "Epoch Loss: 0.016184\n",
      "Epoch Loss: 0.016159\n",
      "Epoch Loss: 0.016143\n",
      "Epoch Loss: 0.016097\n",
      "Epoch Loss: 0.016054\n",
      "Epoch Loss: 0.016014\n",
      "Epoch Loss: 0.015972\n",
      "Epoch Loss: 0.015933\n",
      "Epoch Loss: 0.015906\n",
      "Epoch Loss: 0.015866\n",
      "Epoch Loss: 0.015825\n",
      "Epoch Loss: 0.015785\n",
      "Epoch Loss: 0.015748\n",
      "Epoch Loss: 0.015709\n",
      "Epoch Loss: 0.015670\n",
      "Epoch Loss: 0.015635\n",
      "Epoch Loss: 0.015597\n",
      "Epoch Loss: 0.015559\n",
      "Epoch Loss: 0.015523\n",
      "Epoch Loss: 0.015487\n",
      "Epoch Loss: 0.015450\n",
      "Epoch Loss: 0.015415\n",
      "Epoch Loss: 0.015380\n",
      "Epoch Loss: 0.015343\n",
      "Epoch Loss: 0.015308\n",
      "Epoch Loss: 0.015275\n",
      "Epoch Loss: 0.015238\n",
      "Epoch Loss: 0.015203\n",
      "Epoch Loss: 0.015171\n",
      "Epoch Loss: 0.015135\n",
      "Epoch Loss: 0.015100\n",
      "Epoch Loss: 0.015069\n",
      "Epoch Loss: 0.015042\n",
      "Epoch Loss: 0.015007\n",
      "Epoch Loss: 0.014976\n",
      "Epoch Loss: 0.014941\n",
      "Epoch Loss: 0.014907\n",
      "Epoch Loss: 0.014876\n",
      "Epoch Loss: 0.014842\n",
      "Epoch Loss: 0.014809\n",
      "Epoch Loss: 0.014777\n",
      "Epoch Loss: 0.014745\n",
      "Epoch Loss: 0.014712\n",
      "Epoch Loss: 0.014680\n",
      "Epoch Loss: 0.014649\n",
      "Epoch Loss: 0.014617\n",
      "Epoch Loss: 0.014585\n",
      "Epoch Loss: 0.014555\n",
      "Epoch Loss: 0.014523\n",
      "Epoch Loss: 0.014495\n",
      "Epoch Loss: 0.014474\n",
      "Epoch Loss: 0.014441\n",
      "Epoch Loss: 0.014409\n",
      "Epoch Loss: 0.014380\n",
      "Epoch Loss: 0.014349\n",
      "Epoch Loss: 0.014318\n",
      "Epoch Loss: 0.014288\n",
      "Epoch Loss: 0.014259\n",
      "Epoch Loss: 0.014228\n",
      "Epoch Loss: 0.014198\n",
      "Epoch Loss: 0.014170\n",
      "Epoch Loss: 0.014139\n",
      "Epoch Loss: 0.014110\n",
      "Epoch Loss: 0.014082\n",
      "Epoch Loss: 0.014052\n",
      "Epoch Loss: 0.014022\n",
      "Epoch Loss: 0.014014\n",
      "Epoch Loss: 0.014001\n",
      "Epoch Loss: 0.013977\n",
      "Epoch Loss: 0.013963\n",
      "Epoch Loss: 0.013930\n",
      "Epoch Loss: 0.013898\n",
      "Epoch Loss: 0.013888\n",
      "Epoch Loss: 0.013871\n",
      "Epoch Loss: 0.013836\n",
      "Epoch Loss: 0.013803\n",
      "Epoch Loss: 0.013771\n",
      "Epoch Loss: 0.013741\n",
      "Epoch Loss: 0.013710\n",
      "Epoch Loss: 0.013680\n",
      "Epoch Loss: 0.013653\n",
      "Epoch Loss: 0.013631\n",
      "Epoch Loss: 0.013602\n",
      "Epoch Loss: 0.013579\n",
      "Epoch Loss: 0.013554\n",
      "Epoch Loss: 0.013525\n",
      "Epoch Loss: 0.013498\n",
      "Epoch Loss: 0.013470\n",
      "Epoch Loss: 0.013442\n",
      "Epoch Loss: 0.013414\n",
      "Epoch Loss: 0.013388\n",
      "Epoch Loss: 0.013362\n",
      "Epoch Loss: 0.013334\n",
      "Epoch Loss: 0.013308\n",
      "Epoch Loss: 0.013283\n",
      "Epoch Loss: 0.013257\n",
      "Epoch Loss: 0.013230\n",
      "Epoch Loss: 0.013204\n",
      "Epoch Loss: 0.013180\n",
      "Epoch Loss: 0.013154\n",
      "Epoch Loss: 0.013128\n",
      "Epoch Loss: 0.013104\n",
      "Epoch Loss: 0.013079\n",
      "Epoch Loss: 0.013053\n",
      "Epoch Loss: 0.013029\n",
      "Epoch Loss: 0.013005\n",
      "Epoch Loss: 0.012980\n",
      "Epoch Loss: 0.012956\n",
      "Epoch Loss: 0.012941\n",
      "Epoch Loss: 0.012916\n",
      "Epoch Loss: 0.012891\n",
      "Epoch Loss: 0.012867\n",
      "Epoch Loss: 0.012843\n",
      "Epoch Loss: 0.012818\n",
      "Epoch Loss: 0.012797\n",
      "Epoch Loss: 0.012779\n",
      "Epoch Loss: 0.012754\n",
      "Epoch Loss: 0.012730\n",
      "Epoch Loss: 0.012707\n",
      "Epoch Loss: 0.012682\n",
      "Epoch Loss: 0.012658\n",
      "Epoch Loss: 0.012636\n",
      "Epoch Loss: 0.012612\n",
      "Epoch Loss: 0.012589\n",
      "Epoch Loss: 0.012567\n",
      "Epoch Loss: 0.012544\n",
      "Epoch Loss: 0.012520\n",
      "Epoch Loss: 0.012498\n",
      "Epoch Loss: 0.012476\n",
      "Epoch Loss: 0.012453\n",
      "Epoch Loss: 0.012431\n",
      "Epoch Loss: 0.012409\n",
      "Epoch Loss: 0.012386\n",
      "Epoch Loss: 0.012364\n",
      "Epoch Loss: 0.012343\n",
      "Epoch Loss: 0.012320\n",
      "Epoch Loss: 0.012298\n",
      "Epoch Loss: 0.012277\n",
      "Epoch Loss: 0.012255\n",
      "Epoch Loss: 0.012233\n",
      "Epoch Loss: 0.012212\n",
      "Epoch Loss: 0.012191\n",
      "Epoch Loss: 0.012169\n",
      "Epoch Loss: 0.012148\n",
      "Epoch Loss: 0.012127\n",
      "Epoch Loss: 0.012106\n",
      "Epoch Loss: 0.012090\n",
      "Epoch Loss: 0.012080\n",
      "Epoch Loss: 0.012064\n",
      "Epoch Loss: 0.012042\n",
      "Epoch Loss: 0.012024\n",
      "Epoch Loss: 0.012014\n",
      "Epoch Loss: 0.011991\n",
      "Epoch Loss: 0.011968\n",
      "Epoch Loss: 0.011947\n",
      "Epoch Loss: 0.011928\n",
      "Epoch Loss: 0.011928\n",
      "Epoch Loss: 0.011904\n",
      "Epoch Loss: 0.011881\n",
      "Epoch Loss: 0.011859\n",
      "Epoch Loss: 0.011837\n",
      "Epoch Loss: 0.011815\n",
      "Epoch Loss: 0.011794\n",
      "Epoch Loss: 0.011773\n",
      "Epoch Loss: 0.011752\n",
      "Epoch Loss: 0.011732\n",
      "Epoch Loss: 0.011711\n",
      "Epoch Loss: 0.011691\n",
      "Epoch Loss: 0.011670\n",
      "Epoch Loss: 0.011651\n",
      "Epoch Loss: 0.011631\n",
      "Epoch Loss: 0.011611\n",
      "Epoch Loss: 0.011597\n",
      "Epoch Loss: 0.011578\n",
      "Epoch Loss: 0.011559\n",
      "Epoch Loss: 0.011544\n",
      "Epoch Loss: 0.011529\n",
      "Epoch Loss: 0.011510\n",
      "Epoch Loss: 0.011490\n",
      "Epoch Loss: 0.011471\n",
      "Epoch Loss: 0.011451\n",
      "Epoch Loss: 0.011433\n",
      "Epoch Loss: 0.011414\n",
      "Epoch Loss: 0.011395\n",
      "Epoch Loss: 0.011377\n",
      "Epoch Loss: 0.011359\n",
      "Epoch Loss: 0.011340\n",
      "Epoch Loss: 0.011322\n",
      "Epoch Loss: 0.011304\n",
      "Epoch Loss: 0.011286\n",
      "Epoch Loss: 0.011267\n",
      "Epoch Loss: 0.011250\n",
      "Epoch Loss: 0.011232\n",
      "Epoch Loss: 0.011214\n",
      "Epoch Loss: 0.011197\n",
      "Epoch Loss: 0.011180\n",
      "Epoch Loss: 0.011162\n",
      "Epoch Loss: 0.011149\n",
      "Epoch Loss: 0.011132\n",
      "Epoch Loss: 0.011114\n",
      "Epoch Loss: 0.011097\n",
      "Epoch Loss: 0.011080\n",
      "Epoch Loss: 0.011062\n",
      "Epoch Loss: 0.011045\n",
      "Epoch Loss: 0.011029\n",
      "Epoch Loss: 0.011011\n",
      "Epoch Loss: 0.010994\n",
      "Epoch Loss: 0.010977\n",
      "Epoch Loss: 0.010961\n",
      "Epoch Loss: 0.010943\n",
      "Epoch Loss: 0.010927\n",
      "Epoch Loss: 0.010910\n",
      "Epoch Loss: 0.010893\n",
      "Epoch Loss: 0.010877\n",
      "Epoch Loss: 0.010861\n",
      "Epoch Loss: 0.010844\n",
      "Epoch Loss: 0.010828\n",
      "Epoch Loss: 0.010812\n",
      "Epoch Loss: 0.010795\n",
      "Epoch Loss: 0.010779\n",
      "Epoch Loss: 0.010763\n",
      "Epoch Loss: 0.010747\n",
      "Epoch Loss: 0.010731\n",
      "Epoch Loss: 0.010716\n",
      "Epoch Loss: 0.010700\n",
      "Epoch Loss: 0.010684\n",
      "Epoch Loss: 0.010676\n",
      "Epoch Loss: 0.010663\n",
      "Epoch Loss: 0.010647\n",
      "Epoch Loss: 0.010632\n",
      "Epoch Loss: 0.010616\n",
      "Epoch Loss: 0.010600\n",
      "Epoch Loss: 0.010584\n",
      "Epoch Loss: 0.010569\n",
      "Epoch Loss: 0.010553\n",
      "Epoch Loss: 0.010538\n",
      "Epoch Loss: 0.010523\n",
      "Epoch Loss: 0.010510\n",
      "Epoch Loss: 0.010497\n",
      "Epoch Loss: 0.010482\n",
      "Epoch Loss: 0.010467\n",
      "Epoch Loss: 0.010451\n",
      "Epoch Loss: 0.010436\n",
      "Epoch Loss: 0.010421\n",
      "Epoch Loss: 0.010406\n",
      "Epoch Loss: 0.010391\n",
      "Epoch Loss: 0.010378\n",
      "Epoch Loss: 0.010369\n",
      "Epoch Loss: 0.010354\n",
      "Epoch Loss: 0.010338\n",
      "Epoch Loss: 0.010324\n",
      "Epoch Loss: 0.010309\n",
      "Epoch Loss: 0.010294\n",
      "Epoch Loss: 0.010280\n",
      "Epoch Loss: 0.010272\n",
      "Epoch Loss: 0.010257\n",
      "Epoch Loss: 0.010242\n",
      "Epoch Loss: 0.010227\n",
      "Epoch Loss: 0.010213\n",
      "Epoch Loss: 0.010198\n",
      "Epoch Loss: 0.010183\n",
      "Epoch Loss: 0.010169\n",
      "Epoch Loss: 0.010159\n",
      "Epoch Loss: 0.010145\n",
      "Epoch Loss: 0.010131\n",
      "Epoch Loss: 0.010116\n",
      "Epoch Loss: 0.010102\n",
      "Epoch Loss: 0.010088\n",
      "Epoch Loss: 0.010080\n",
      "Epoch Loss: 0.010072\n",
      "Epoch Loss: 0.010057\n",
      "Epoch Loss: 0.010043\n",
      "Epoch Loss: 0.010028\n",
      "Epoch Loss: 0.010013\n",
      "Epoch Loss: 0.009999\n",
      "Epoch Loss: 0.009985\n",
      "Epoch Loss: 0.009971\n",
      "Epoch Loss: 0.009957\n",
      "Epoch Loss: 0.009943\n",
      "Epoch Loss: 0.009929\n",
      "Epoch Loss: 0.009915\n",
      "Epoch Loss: 0.009902\n",
      "Epoch Loss: 0.009888\n",
      "Epoch Loss: 0.009874\n",
      "Epoch Loss: 0.009861\n",
      "Epoch Loss: 0.009847\n",
      "Epoch Loss: 0.009834\n",
      "Epoch Loss: 0.009821\n",
      "Epoch Loss: 0.009807\n",
      "Epoch Loss: 0.009794\n",
      "Epoch Loss: 0.009781\n",
      "Epoch Loss: 0.009768\n",
      "Epoch Loss: 0.009755\n",
      "Epoch Loss: 0.009741\n",
      "Epoch Loss: 0.009728\n",
      "Epoch Loss: 0.009716\n",
      "Epoch Loss: 0.009702\n",
      "Epoch Loss: 0.009690\n",
      "Epoch Loss: 0.009681\n",
      "Epoch Loss: 0.009668\n",
      "Epoch Loss: 0.009654\n",
      "Epoch Loss: 0.009642\n",
      "Epoch Loss: 0.009629\n",
      "Epoch Loss: 0.009616\n",
      "Epoch Loss: 0.009603\n",
      "Epoch Loss: 0.009591\n",
      "Epoch Loss: 0.009578\n",
      "Epoch Loss: 0.009565\n",
      "Epoch Loss: 0.009553\n",
      "Epoch Loss: 0.009540\n",
      "Epoch Loss: 0.009528\n",
      "Epoch Loss: 0.009516\n",
      "Epoch Loss: 0.009503\n",
      "Epoch Loss: 0.009491\n",
      "Epoch Loss: 0.009478\n",
      "Epoch Loss: 0.009468\n",
      "Epoch Loss: 0.009462\n",
      "Epoch Loss: 0.009450\n",
      "Epoch Loss: 0.009437\n",
      "Epoch Loss: 0.009425\n",
      "Epoch Loss: 0.009412\n",
      "Epoch Loss: 0.009400\n",
      "Epoch Loss: 0.009388\n",
      "Epoch Loss: 0.009375\n",
      "Epoch Loss: 0.009363\n",
      "Epoch Loss: 0.009351\n",
      "Epoch Loss: 0.009340\n",
      "Epoch Loss: 0.009331\n",
      "Epoch Loss: 0.009319\n",
      "Epoch Loss: 0.009307\n",
      "Epoch Loss: 0.009295\n",
      "Epoch Loss: 0.009289\n",
      "Epoch Loss: 0.009277\n",
      "Epoch Loss: 0.009264\n",
      "Epoch Loss: 0.009252\n",
      "Epoch Loss: 0.009240\n",
      "Epoch Loss: 0.009228\n",
      "Epoch Loss: 0.009216\n",
      "Epoch Loss: 0.009205\n",
      "Epoch Loss: 0.009193\n",
      "Epoch Loss: 0.009181\n",
      "Epoch Loss: 0.009169\n",
      "Epoch Loss: 0.009158\n",
      "Epoch Loss: 0.009146\n",
      "Epoch Loss: 0.009134\n",
      "Epoch Loss: 0.009123\n",
      "Epoch Loss: 0.009111\n",
      "Epoch Loss: 0.009100\n",
      "Epoch Loss: 0.009089\n",
      "Epoch Loss: 0.009077\n",
      "Epoch Loss: 0.009066\n",
      "Epoch Loss: 0.009055\n",
      "Epoch Loss: 0.009044\n",
      "Epoch Loss: 0.009032\n",
      "Epoch Loss: 0.009021\n",
      "Epoch Loss: 0.009011\n",
      "Epoch Loss: 0.009004\n",
      "Epoch Loss: 0.008992\n",
      "Epoch Loss: 0.008981\n",
      "Epoch Loss: 0.008970\n",
      "Epoch Loss: 0.008959\n",
      "Epoch Loss: 0.008948\n",
      "Epoch Loss: 0.008937\n",
      "Epoch Loss: 0.008926\n",
      "Epoch Loss: 0.008914\n",
      "Epoch Loss: 0.008904\n",
      "Epoch Loss: 0.008895\n",
      "Epoch Loss: 0.008885\n",
      "Epoch Loss: 0.008874\n",
      "Epoch Loss: 0.008863\n",
      "Epoch Loss: 0.008852\n",
      "Epoch Loss: 0.008841\n",
      "Epoch Loss: 0.008831\n",
      "Epoch Loss: 0.008820\n",
      "Epoch Loss: 0.008809\n",
      "Epoch Loss: 0.008799\n",
      "Epoch Loss: 0.008788\n",
      "Epoch Loss: 0.008777\n",
      "Epoch Loss: 0.008767\n",
      "Epoch Loss: 0.008756\n",
      "Epoch Loss: 0.008745\n",
      "Epoch Loss: 0.008735\n",
      "Epoch Loss: 0.008725\n",
      "Epoch Loss: 0.008714\n",
      "Epoch Loss: 0.008704\n",
      "Epoch Loss: 0.008694\n",
      "Epoch Loss: 0.008683\n",
      "Epoch Loss: 0.008673\n",
      "Epoch Loss: 0.008663\n",
      "Epoch Loss: 0.008652\n",
      "Epoch Loss: 0.008642\n",
      "Epoch Loss: 0.008632\n",
      "Epoch Loss: 0.008622\n",
      "Epoch Loss: 0.008611\n",
      "Epoch Loss: 0.008601\n",
      "Epoch Loss: 0.008591\n",
      "Epoch Loss: 0.008581\n",
      "Epoch Loss: 0.008571\n",
      "Epoch Loss: 0.008564\n",
      "Epoch Loss: 0.008553\n",
      "Epoch Loss: 0.008543\n",
      "Epoch Loss: 0.008534\n",
      "Epoch Loss: 0.008523\n",
      "Epoch Loss: 0.008513\n",
      "Epoch Loss: 0.008504\n",
      "Epoch Loss: 0.008494\n",
      "Epoch Loss: 0.008483\n",
      "Epoch Loss: 0.008474\n",
      "Epoch Loss: 0.008464\n",
      "Epoch Loss: 0.008454\n",
      "Epoch Loss: 0.008448\n",
      "Epoch Loss: 0.008441\n",
      "Epoch Loss: 0.008431\n",
      "Epoch Loss: 0.008422\n",
      "Epoch Loss: 0.008413\n",
      "Epoch Loss: 0.008403\n",
      "Epoch Loss: 0.008393\n",
      "Epoch Loss: 0.008383\n",
      "Epoch Loss: 0.008373\n",
      "Epoch Loss: 0.008368\n",
      "Epoch Loss: 0.008364\n",
      "Epoch Loss: 0.008358\n",
      "Epoch Loss: 0.008348\n",
      "Epoch Loss: 0.008337\n",
      "Epoch Loss: 0.008327\n",
      "Epoch Loss: 0.008317\n",
      "Epoch Loss: 0.008307\n",
      "Epoch Loss: 0.008297\n",
      "Epoch Loss: 0.008287\n",
      "Epoch Loss: 0.008277\n",
      "Epoch Loss: 0.008267\n",
      "Epoch Loss: 0.008258\n",
      "Epoch Loss: 0.008248\n",
      "Epoch Loss: 0.008238\n",
      "Epoch Loss: 0.008229\n",
      "Epoch Loss: 0.008219\n",
      "Epoch Loss: 0.008210\n",
      "Epoch Loss: 0.008201\n",
      "Epoch Loss: 0.008194\n",
      "Epoch Loss: 0.008185\n",
      "Epoch Loss: 0.008175\n",
      "Epoch Loss: 0.008166\n",
      "Epoch Loss: 0.008157\n",
      "Epoch Loss: 0.008147\n",
      "Epoch Loss: 0.008138\n",
      "Epoch Loss: 0.008129\n",
      "Epoch Loss: 0.008120\n",
      "Epoch Loss: 0.008110\n",
      "Epoch Loss: 0.008102\n",
      "Epoch Loss: 0.008092\n",
      "Epoch Loss: 0.008083\n",
      "Epoch Loss: 0.008074\n",
      "Epoch Loss: 0.008065\n",
      "Epoch Loss: 0.008056\n",
      "Epoch Loss: 0.008047\n",
      "Epoch Loss: 0.008039\n",
      "Epoch Loss: 0.008030\n",
      "Epoch Loss: 0.008021\n",
      "Epoch Loss: 0.008012\n",
      "Epoch Loss: 0.008003\n",
      "Epoch Loss: 0.007994\n",
      "Epoch Loss: 0.007985\n",
      "Epoch Loss: 0.007977\n",
      "Epoch Loss: 0.007968\n",
      "Epoch Loss: 0.007959\n",
      "Epoch Loss: 0.007951\n",
      "Epoch Loss: 0.007943\n",
      "Epoch Loss: 0.007935\n",
      "Epoch Loss: 0.007927\n",
      "Epoch Loss: 0.007918\n",
      "Epoch Loss: 0.007909\n",
      "Epoch Loss: 0.007901\n",
      "Epoch Loss: 0.007892\n",
      "Epoch Loss: 0.007884\n",
      "Epoch Loss: 0.007875\n",
      "Epoch Loss: 0.007867\n",
      "Epoch Loss: 0.007858\n",
      "Epoch Loss: 0.007850\n",
      "Epoch Loss: 0.007844\n",
      "Epoch Loss: 0.007836\n",
      "Epoch Loss: 0.007828\n",
      "Epoch Loss: 0.007820\n",
      "Epoch Loss: 0.007811\n",
      "Epoch Loss: 0.007802\n",
      "Epoch Loss: 0.007794\n",
      "Epoch Loss: 0.007786\n",
      "Epoch Loss: 0.007777\n",
      "Epoch Loss: 0.007769\n",
      "Epoch Loss: 0.007761\n",
      "Epoch Loss: 0.007752\n",
      "Epoch Loss: 0.007744\n",
      "Epoch Loss: 0.007736\n",
      "Epoch Loss: 0.007728\n",
      "Epoch Loss: 0.007719\n",
      "Epoch Loss: 0.007711\n",
      "Epoch Loss: 0.007703\n",
      "Epoch Loss: 0.007695\n",
      "Epoch Loss: 0.007687\n",
      "Epoch Loss: 0.007679\n",
      "Epoch Loss: 0.007671\n",
      "Epoch Loss: 0.007662\n",
      "Epoch Loss: 0.007655\n",
      "Epoch Loss: 0.007646\n",
      "Epoch Loss: 0.007638\n",
      "Epoch Loss: 0.007630\n",
      "Epoch Loss: 0.007622\n",
      "Epoch Loss: 0.007614\n",
      "Epoch Loss: 0.007606\n",
      "Epoch Loss: 0.007601\n",
      "Epoch Loss: 0.007593\n"
     ]
    }
   ],
   "source": [
    "# Training Procedure\n",
    "# Initialize the optimizer. DO NOT change any of the hyper-parameters here or above.\n",
    "# We have implemented the SGD optimizer class for you here, which visits each layer sequentially to\n",
    "# get the gradients and optimize the respective parameters.\n",
    "# You should work with the given parameters and only edit your implementation in the .py files\n",
    "\n",
    "epochs = 1000\n",
    "optim = SGD(net, lr=0.1, weight_decay=0.00001)\n",
    "\n",
    "epoch_loss = []\n",
    "for epoch in range(epochs):\n",
    "    # Get output scores from the network\n",
    "    output_x = net(X)\n",
    "    # Calculate the loss for these output scores, given the true labels\n",
    "    loss = Loss.forward(output_x, y)\n",
    "    # Initialize your gradients to None in each epoch\n",
    "    optim.zero_grad()\n",
    "    # Make a backward pass to update the internal gradients in the layers\n",
    "    net.backward(Loss.backward())\n",
    "    # call the step function in the optimizer to update the values of the params with the gradients\n",
    "    optim.step()\n",
    "    # Append the loss at each iteration\n",
    "    epoch_loss.append(loss)\n",
    "\n",
    "    print(\"Epoch Loss: {:3f}\".format(epoch_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 2 0 0 2 0 0]\n",
      "[2 1 0 1 2 0 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Test your predictions. The predictions must match the labels\n",
    "print(net.predict(X))\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss 0.007593419801731252\n"
     ]
    }
   ],
   "source": [
    "# You should be able to achieve a training loss of less than 0.02 (10%)\n",
    "print(\"Final training loss\", epoch_loss[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss curve. The loss in the curve should be decreasing (20%)\n",
    "plt.plot(epoch_loss)\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
